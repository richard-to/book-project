[main]
# Change to 0 for full production ETL
limit_records=0

# Number of paritions to use when writing data to S3 in parquet
num_write_partitions=48

[aws]
key=
secret=

[goodreads]
# https://www.kaggle.com/jealousleopard/goodreadsbooks
data_path=s3://bucket/goodreads-data/books.csv

[output]
br_book_author_path=s3://bucket/br_book_author
br_book_subject_path=s3://bucket/br_book_subject
dim_author_path=s3://bucket/dim_author
dim_book_path=s3://bucket/dim_book
dim_checkout_time_path=s3://bucket/dim_checkout_time
dim_publisher_path=s3://bucket/dim_publisher
dim_subject_path=s3://bucket/dim_subject
fact_spl_book_checkout_path=s3://bucket/fact_spl_book_checkout

[publishers]
# Used for loading "official" publishers into elasticsearch, which
# will be used for deduplicating raw publisher data
elasticsearch_host=localhost
# This file is generated by create_publisher_mapping.py script
data_path=publisher-mapping/publishers-map.csv

[redshift]
cluster_type=multi-node
num_nodes=4
node_type=dc2.large
db_name=spl
db_user=spl_user
db_password=
db_port=5439
iam_role_name=splRole
cluster_identifier=splCluster

[spl]
# https://www.kaggle.com/seattle-public-library/seattle-library-checkout-records
checkouts_path=s3://bucket/spl-data/Checkouts_By_Title_Data_Lens_*.csv
data_dict_path=s3://bucket/spl-data/Integrated_Library_System__ILS__Data_Dictionary.csv
inventory_path=s3://bucket/spl-data/Library_Collection_Inventory.csv

[weather]
# https://academic.udayton.edu/kissock/http/Weather/gsod95-current/WASEATTL.txt
data_path=s3://bucket/weather-data/WASEATTL.txt
